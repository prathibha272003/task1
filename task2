# Install required libraries
# pip install requests beautifulsoup4 langchain sentence-transformers faiss-cpu openai

import requests
from bs4 import BeautifulSoup
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# **Step 1: Data Ingestion**
# Scrape website content
def scrape_website(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        # Extract textual content from paragraphs and headings
        paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        content = " ".join([para.get_text(strip=True) for para in paragraphs])
        return content
    else:
        raise Exception(f"Failed to retrieve {url}. Status code: {response.status_code}")

# Chunk content into manageable sections
def chunk_text(text, chunk_size=500):
    chunks = []
    words = text.split()
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

# Convert chunks to embeddings
def embed_and_store(chunks):
    embeddings_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    vector_store = FAISS.from_texts(chunks, embeddings_model)
    return vector_store

# **Step 2: Query Handling and Response Generation**
def query_website(vector_store, query, api_key):
    llm = OpenAI(model="text-davinci-003", openai_api_key=api_key)
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vector_store.as_retriever())
    response = qa_chain.run(query)
    return response

# **Step 3: Example Usage**
if __name__ == "__main__":
    # Input website URL
    website_url = "https://www.example.com"  # Replace with the target URL
    api_key = "your-openai-api-key"  # Replace with your OpenAI API key

    # Step 1: Scrape, chunk, and embed website data
    print(f"Scraping website: {website_url}")
    try:
        website_content = scrape_website(website_url)
        text_chunks = chunk_text(website_content)
        vector_store = embed_and_store(text_chunks)
        print("Website data stored in vector database.")
    except Exception as e:
        print(e)
        exit()

    # Step 2: Query the data
    user_query = "What does the website say about its services?"  # Example query
    print("\nQuerying website data...")
    response = query_website(vector_store, user_query, api_key)
    print("\nResponse:")
    print(response)
